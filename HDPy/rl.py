"""
The Reinforcement Learning Problem is approached by means of an
Actor-Critic design. This method splits the agent into a
return-estimator (Critic) and an action-selection mechanism (Actor).
Information about state and reward is provided by the plant to the
agent. As the agent is still viewed as one unit, both of its parts are
embedded in the same class, the :py:class:`ActorCritic`. It does not
itself implement a method to solve the learning problem but only
provides preliminaries for an algorithm doing so. Meaning, that it
defines common members and method interfaces. Furthermore, it binds
the Actor-Critic approach to a :py:class:`PuPy.RobotActor`, such that
any of its descendants can be used within :py:mod:`PuPy`.

The Actor-Critic implementation is kept general, meaning that it is not
limited to a specific learning problem. For this, the template classes
:py:class:`Plant` and :py:class:`Policy` are defined. Using the former,
a concrete environment can be implemented by specifying state and reward.
The latter class is required to hide the representation of the action
from the :py:class:`ActorCritic`. Due to the integration in
:py:mod:`PuPy`, motor targets (a low-level representation of an
action) have to be generated, but the action representation for the
Reinforcement Learning problem may be more abstract. For example, gait
parameters could be used as action. From these, a motor target sequence
has to be generated to actually steer the robot.

change log:
    02.06.2014: ActorCritic.num_step is increased in the beginning of __call__ (not in the end)
    03.06.2014: Removed ActorCritic.init_episode(). Instead use ActorCritic.step_init(epoch) to
                define the behavior during the initial steps.
                ActorCritic._step is changed to have only arguments ActorCritic._step(epoch, reward)
    18.06.2014: reward is written to epoch in rl.ActorCritic
    18.06.2014: rl.ActorCritic.policy is new alias for rl.ActorCritic.child (for convenience and back-compatibility)
"""
import PuPy
import numpy as np
import cPickle as pickle

class Plant(object):
    """A template for Actor-Critic *plants*. The *Plant* describes the
    interaction of the Actor-Critic with the environment. Given a robot
    which follows a certain *Policy*, the environment generates rewards
    and robot states.
    
    An additional instance to :py:class:`PuPy.Normalization` may be 
    supplied in ``norm`` for normalizing sensor values.
    """
    def __init__(self, state_space_dim=None, norm=None, verbose=False):
        self._state_space_dim = state_space_dim
        self.normalization = None
        self.set_normalization(norm)
        self.verbose = verbose
    
    def state_input(self, state):
        """Return the state-part of the critic input
        (i.e. the reservoir input).
        
        The state-part is derived from the current robot ``state`` and
        possibly also its ``action``. As return format, a Nx1 numpy
        vector is expected, where 2 dimensions should exist (e.g.
        :py:meth:`numpy.atleast_2d`).
        
        Although the reservoir input will consist of both, the *state*
        and *action*, this method must only return the *state* part of
        it.
        
        """
        raise NotImplementedError()
    
    def reward(self, epoch):
        """A reward generated by the *Plant* based on the current
        sensor readings in ``epoch``. The reward is single-dimensional.
        
        The reward is evaluated in every step. It builds the foundation
        of the approximated return.
        """
        raise NotImplementedError()
    
    def state_space_dim(self):
        """Return the dimension of the state space.
        This value is equal to the size of the vector returned by
        :py:meth:`state_input`.
        """
        if self._state_space_dim is None:
            raise NotImplementedError()
        return self._state_space_dim
    
    def set_normalization(self, norm):
        """Set the normalization instance to ``norm``."""
        if norm is None:
            norm = PuPy.Normalization()
        self.normalization = norm
    
    def reset(self):
        """Reset plant to initial state."""
        pass

class Policy(PuPy.RobotActor):
    """A template for Actor-Critic *policies*. The *Policy* defines how
    an action is translated into a control (motor) signal. It
    continously receives action updates from the *Critic* which it has
    to digest.
    
    An additional instance to :py:class:`PuPy.Normalization` may be 
    supplied in ``norm`` for normalizing sensor values.
    """
    def __init__(self, action_space_dim=None, norm=None, **kwargs):
        super(Policy, self).__init__(**kwargs)
        self._action_space_dim = action_space_dim
        self.normalization = None
        self.set_normalization(norm)
    
    def initial_action(self):
        """Return the initial action. A valid action must be returned
        since the :py:class:`ActorCritic` relies on the format.
        
        The action has to be a 2-dimensional numpy vector, with both
        dimensions available.
        """
        raise NotImplementedError()
    
    def update(self, action_upd):
        """Update the *Policy* according to the current action update
        ``action_upd``, which was in turn computed by the
        :py:class:`ActorCritic`.
        """
        raise NotImplementedError()
    
    def get_iterator(self, time_start_ms, time_end_ms, step_size_ms):
        """Return an iterator for the *motor_target* sequence, according
        to the current action configuration.
        
        The *motor_targets* glue the *Policy* and *Plant* together.
        Since they are applied in the robot and effect the sensor
        readouts, they are an "input" to the environment. As the targets
        are generated as effect of the action update, they are an output
        of the policy.
        
        """
        raise NotImplementedError()
    
    def action_space_dim(self):
        """Return the dimension of the action space.
        This value is equal to the size of the vector returned by
        :py:meth:`initial_action`.
        """
        if self._action_space_dim is None:
            raise NotImplementedError()
        return self._action_space_dim
    
    def reset(self):
        """Undo any policy updates."""
        raise NotImplementedError()
    
    def set_normalization(self, norm):
        """Set the normalization instance to ``norm``."""
        if norm is None:
            norm = PuPy.Normalization()
        self.normalization = norm
    
    
    def __call__(self, epoch, time_start_ms, time_end_ms, step_size_ms):
        if epoch.has_key('a_next'):
            self.update(np.atleast_2d(epoch['a_next']).T)
        return self.get_iterator(time_start_ms, time_end_ms, step_size_ms)
    
    def _get_initial_targets(self, time_start_ms, time_end_ms, step_size_ms):
        return self.__call__({}, time_start_ms, time_end_ms, step_size_ms)
        

class _ConstParam(object):
    """Stub for wrapping constant values into an executable function."""
    def __init__(self, value):
        self._value = value
    def __call__(self, time0=None, time1=None):
        """Return the constant value."""
        return self._value

class Momentum(object):
    """Template class for an action momentum. 
    
    With a momentum, the next action is computed from the lastest one
    and the proposed action :math:`a^*`. The momentum controls how much
    each of the two influences the next action. Generally, a momentum
    of zero implies following strictly the proposal, while a momentum
    of one does the opposite. Usually, the (linear) momentum is
    formulated as
    
    .. math::
        a_{t+1} = m a_t + (1-m) a^*
    
    The momentum may be time dependent with
    - time0: Episode counter
    - time1: Episode's step counter
    
    """
    def __call__(self, a_curr, a_prop, time0=None, time1=None):
        """Return the next action from a current action ``a_curr``, 
        a proposal ``a_prop`` at episode ``time0`` in step ``time1``."""
        raise NotImplementedError()

class ConstMomentum(Momentum):
    """Linear momentum equation, as specified in :py:class:`Momentum`
    with time-constant momentum value (m).
    
    ``value``
        Momentum value, [0,1].
    
    """
    def __init__(self, value):
        super(ConstMomentum, self).__init__()
        self._value = value
        assert 0 <= self._value and self._value <= 1
    
    def __call__(self, a_curr, a_prop, time0=None, time1=None):
        """Return the next action from a current action ``a_curr``, 
        a proposal ``a_prop`` at episode ``time0`` in step ``time1``."""
        return self._value * a_curr + (1.0 - self._value) * a_prop

class RadialMomentum(Momentum):
    """Momentum with respect to angular action. The resulting action
    is the (smaller) intermediate angle of the latest action
    and proposal (with respect to the momentum). The actions are
    supposed to be in radians, hence the output is in the range
    :math:`[0,2\pi]`. The momentum is a time-constant value (m).
    
    ``value``
        Momentum value, [0,1].
    
    """
    def __init__(self, value):
        super(RadialMomentum, self).__init__()
        self._value = value
        assert 0 <= self._value and self._value <= 1
    
    def __call__(self, a_curr, a_prop, time0=None, time1=None):
        """Return the next action from a current action ``a_curr``, 
        a proposal ``a_prop`` at episode ``time0`` in step ``time1``."""
        phi_0 = a_curr % (2*np.pi)
        phi_1 = a_prop % (2*np.pi)
        imag_0 = np.exp(phi_0*1j)
        imag_1 = np.exp(phi_1*1j)
        imag_r = self._value * imag_0 + (1.0 - self._value) * imag_1
        return np.angle(imag_r) % (2*np.pi)

class ActorCritic(PuPy.RobotActor):
    """Actor-critic design.
    
    The Actor-Critic estimates the return function
    
    .. math::
        J_t = \sum\limits_{k=t}^{T} \gamma^k r_{t+k+1}
    
    while the return is optimized at the same time. This is done by
    incrementally updating the estimate for :math:`J_t` and choosing
    the next action by optimizing the return in a single step. See
    [ESN-ACD]_ for details.
    
    ``plant``
        An instance of :py:class:`Plant`. The plant defines the
        interaction with the environment.
    
    ``child``
        An instance of :py:class:`RobotActor` which should be a
        :py:class:`Policy` or have one as child. The policy defines the
        interaction with the robot's actuators.
    
    ``gamma``
        Choice of *gamma* in the return function. May be a constant or
        a function of the time (relative to the episode start).
        
    ``alpha``
        Choice of *alpha* in the action update. May be a constant or a
        function of the time (relative to the episode start).
        
        The corresponding formula is
        
        .. math::
            a_{t+1} = a_{t} + \\alpha \\frac{\partial J_t}{\partial a_t}
        
        See [ESN-ACD]_ for details.
        
    ``norm``
        A :py:class:`PuPy.Normalization` for normalization purposes.
        Note that the parameters for *a_curr* and *a_next* should be
        exchangable, since it's really the same kind of 'sensor'.
    
    Important members of the :py:class:`ActorCritic` are:
        
        ``s_curr``
            Last observed state. :py:keyword:`dict`, same as ``epoch``
            of the :py:meth:`__call__`.
        
        ``a_curr``
            Last executed action. This is the action which lead
            from ``s_curr`` into ``epoch``. Type specified through
            the :py:class:`Policy`.
    
    """
    def __init__(self, plant, policy, gamma=1.0, alpha=1.0, init_steps=1, norm=None, momentum=0.0, **kwargs):
        super(ActorCritic, self).__init__(child=policy, **kwargs)
        self.policy = self.child # refer to child as policy 
        
        # Initial members
        self.plant = plant
        self.normalizer = None
        self.num_episode = 0
        self._init_steps = init_steps
        self.a_curr = None
        self._action_space_dim = None
        self.s_curr = None
        self.alpha = None
        self.momentum = None
        self.gamma = None
        self.num_step = 0
        
        # Init members through dedicated routines
        self.set_normalization(norm)
        self.set_alpha(alpha)
        self.set_gamma(gamma)
        self.set_momentum(momentum)
        
        # Check assumptions
        assert self.policy.initial_action().shape[0] >= 1
        assert self.policy.initial_action().shape[1] == 1
        
        # Start a new episode
        self.new_episode()
    
    def new_episode(self):
        """Start a new episode of the same experiment. This method can
        also be used to initialize the *ActorCritic*, for example when
        it is loaded from a file.
        """
        self.plant.reset()
        self.policy.reset()
        self._action_space_dim = self.policy.action_space_dim()
        self.a_curr = self.policy.initial_action()
        if self.verbose: print '\nt:    0 n: 0 (new episode) a_curr:', self.a_curr.T
        self.s_curr = dict()
        self.num_episode += 1
        self.num_step = 0
    
    def __call__(self, epoch, time_start_ms, time_end_ms, step_size_ms):
        """One round in the actor-critic cycle. The current observations
        are given in ``epoch`` and the timing information in the rest of
        the parameters. For a detailed description of the parameters,
        see :py:class:`PuPy.PuppyActor`.
        
        This routine computes the reward from the *epoch* and manages
        consecutive epochs, then lets :py:meth:`_step` compute the next
        action.
        
        """
        self.num_step += 1
        if self.verbose: print 't:', time_start_ms, 'n:', self.num_step, 
        epoch_raw = epoch.copy() # store only raw sensor values in self.s_curr
        if self.num_step <= self._init_steps-1:
            
            epoch = self._step_init(epoch)
            
            if self.verbose:
                print '(init)',
            
        else:       
            # extern through the robot:
            # take action (a_curr = a_next in the previous run)
            # observe sensors values produced by the action (a_curr = previous a_next)
            
            # Generate reinforcement signal U(k), given in(k)
            reward = self.plant.reward(epoch)
            #reward = self.plant.reward(self.s_curr)
            # Matthias: It's not clear, which reward should be the input to the critic:
            # While the ACD papers imply the reward of time step n, the book
            # by Sutton/Barto indicate the reward as being from the next
            # state, n+1. Experiments indicate that it doesn't really matter.
            # To be consistent with other work, I go with time n.
            # Nico: changed it to be epoch (n+1). It should be the reward of performing
            # action a_n when being in state s_n. The reward is calculated on the 
            # outcome of that, so on the new state s_n+1 (which is epoch).
            if self.verbose:
                print 'reward:', reward,
            
            # do the actual work
            epoch = self._step(epoch, reward)
            
            # save reward
            epoch['reward'] = np.atleast_2d([reward]).T
        
        # increment
        epoch['a_curr'] = self.a_curr.T
        self.a_curr = np.atleast_2d(epoch['a_next']).T
        self.s_curr = epoch_raw
        self._pre_increment_hook(epoch)
        
        if self.verbose:
            print 'a_next:', epoch['a_next']
        
        # return next action
        return self.child(epoch, time_start_ms, time_end_ms, step_size_ms)
    
    def _step_init(self, epoch):
        """Define the behaviour during the initial phase, i.e. as long
        as
        
            num_step <= init_steps
        
        with ``num_step`` the episode's step iterator and ``init_steps``
        given at construction (default 1). The default is to store the
        ``epoch`` but do nothing else.
        """
        epoch['a_next'] = self.a_curr.T
        return epoch
    
    def _step(self, epoch, reward):
        """Execute one step of the actor and return the next action.
        
        When overloading this method, it must be ensured that
        :py:meth:`_next_action_hook` is executed as soon as the next
        action is determined and also :py:meth:`_pre_increment_hook`
        should be called before the method returns (passing relevant
        intermediate results).
        
        ``s_curr``
            Previous observed state. :py:keyword:`dict`, same as ``epoch``
            of the :py:meth:`__call__`.
        
        ``epoch``
            Latest observed state. :py:keyword:`dict`, same as ``epoch``
            of the :py:meth:`__call__`.
        
        ``a_curr``
            Previously executed action. This is the action which lead
            from ``s_curr`` into ``s_next``. Type specified through
            the :py:class:`Policy`.
        
        ``reward``
            Reward of ``s_next``
        
        """
        raise NotImplementedError()
    
    def _pre_increment_hook(self, epoch, **kwargs):
        """Template method for subclasses.
        
        Before the actor-critic cycle increments, this method is invoked
        with all relevant locals of the :py:meth:`ADHDP.__call__`
        method.
        """
        pass
    
    def _next_action_hook(self, a_next):
        """Postprocessing hook, after the next action ``a_next`` was
        proposed by the algorithm. Must return the possibly altered
        next action in the same format."""
        return a_next
    
    def save(self, pth):
        """Store the current instance in a file at ``pth``.
        
        .. note::
            If ``alpha`` or ``gamma`` was set to a user-defined
            function, make sure it's pickable. Especially, anonymous
            functions (:keyword:`lambda`) can't be pickled.
        
        """
        child = self.child
        self.child = None
        self.policy = None
        
        f = open(pth, 'w')
        pickle.dump(self, f)
        f.close()
        
        self.child = child
        self.policy = self.child
    
    @staticmethod
    def load(pth):
        """Load an instance from a file ``pth``.
        """
        f = open(pth, 'r')
        cls = pickle.load(f)
        cls.new_episode()
        return cls
    
    def set_alpha(self, alpha):
        """Define a value for ``alpha``. May be either a constant or
        a function of the time.
        """
        if callable(alpha):
            self.alpha = alpha
        else:
            self.alpha = _ConstParam(alpha)
    
    def set_gamma(self, gamma):
        """Define a value for ``gamma``. May be either a constant or
        a function of the time.
        """
        if callable(gamma):
            self.gamma = gamma
        else:
            self.gamma = _ConstParam(gamma)
    
    def set_momentum(self, momentum):
        """Define a value for ``momentum``. May be either a constant or
        a function of the time.
        """
        if callable(momentum):
            self.momentum = momentum
        else:
            self.momentum = ConstMomentum(momentum)
    
    def set_normalization(self, norm):
        """Set the normalization instance to ``norm``. The normalization
        is propagated to the plant and policy."""
        if norm is None:
            norm = PuPy.Normalization()
        self.normalizer = norm
        self.plant.set_normalization(norm)
        self.policy.set_normalization(norm)

